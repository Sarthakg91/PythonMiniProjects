{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment III – CS 8803 Data Analytics for Well-Being \n",
    "\n",
    "                                                                              Submitted by :Sarthak Ghosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A\n",
    "\n",
    "#### Create feature vectors for mental well being where each feature vector contains all the answers for all the questions in a particular order. First find avergae of pre and post values for flourishing scale and write it to a file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pre scores 46\n",
      "Length of post scores 37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "FlourishingScale=open('data/Surveys/FlourishingScale.csv').read().splitlines()\n",
    "flourishing_responses_pre=[]\n",
    "flourishing_responses_post=[]\n",
    "\n",
    "for response in FlourishingScale[1:]:\n",
    "    \n",
    "    #there are some blank values, then replace the blanks with 0\n",
    "    while((',,') in response):\n",
    "        response=response.replace(',,',',0,')\n",
    "        \n",
    "    temp_response=response.split(',')\n",
    "    if('pre' in temp_response):\n",
    "        flourishing_responses_pre.append(temp_response)\n",
    "    else:\n",
    "        flourishing_responses_post.append(temp_response)\n",
    "        \n",
    "print \"Length of pre scores\", len(flourishing_responses_pre)   \n",
    "print \"Length of post scores\", len(flourishing_responses_post)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineScores(list_a,list_b):\n",
    "    return np.mean([[list_a], [list_b]], axis=0, dtype = int64).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineScores2(list1,list2):\n",
    "    list1 = ['0' if item=='' else item for item in list1]\n",
    "    list2 = ['0' if item=='' else item for item in list2]\n",
    "    combinedScoreList=[list1[0]]\n",
    "    for i in range(len(list1[2:])):\n",
    "            sums=float(list1[i+2])+float(list2[i+2])\n",
    "            avg=sums/2.0 \n",
    "            combinedScoreList.append(str(avg))         \n",
    "    return combinedScoreList\n",
    "\n",
    "def createAverageScores(pre,post):\n",
    "    combined_scores = []\n",
    "    for pre_response in pre:\n",
    "        exists=0\n",
    "        for post_response in post:\n",
    "            if pre_response[0]==post_response[0]:\n",
    "                temp_combined_scores=combineScores2(pre_response,post_response)\n",
    "                combined_scores.append(temp_combined_scores)\n",
    "                exists=1\n",
    "        if exists==0:   \n",
    "            combined_scores.append(pre_response[:1]+pre_response[2:]) \n",
    "    return combined_scores\n",
    "\n",
    "def fileWrite(filepath,combined_scores):\n",
    "    \n",
    "    with open(filepath, \"w\") as newFlourishingFile:\n",
    "    \n",
    "        num_cols=len(combined_scores[0])\n",
    "\n",
    "        newFlourishingFile.write('uid,'+','.join(str(item) for item in range(1,num_cols))+'\\n')\n",
    "\n",
    "        for i in range(len(combined_scores)):\n",
    "            for j in range(len(combined_scores[i])):\n",
    "                newFlourishingFile.write(str(combined_scores[i][j])\n",
    "                                         + \",\");\n",
    "            newFlourishingFile.write('\\n');\n",
    "\n",
    "combined_scores=createAverageScores(flourishing_responses_pre,flourishing_responses_post)\n",
    "fileWrite('data/Surveys/FlourishingScaleAVG.csv',\n",
    "          combined_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "#doing the same thing with the loneliness file. Read the values, change the strings into numbers. find average of pre and post values and\n",
    "#write into another file\n",
    "\n",
    "value_map= {'never': '0', 'rarely': '1','sometimes': '2', 'often': '3'}\n",
    "loneliness_values=open('data/Surveys/LonelinessScale.csv').read().splitlines()\n",
    "loneliness_responses_pre=[]\n",
    "loneliness_responses_post=[]\n",
    "\n",
    "for i in range(len(loneliness_values)):\n",
    "    loneliness_values[i]=loneliness_values[i].replace('Never',value_map['never'])\n",
    "    loneliness_values[i]=loneliness_values[i].replace('Rarely',value_map['rarely'])\n",
    "    loneliness_values[i]=loneliness_values[i].replace('Sometimes',value_map['sometimes'])\n",
    "    loneliness_values[i]=loneliness_values[i].replace('Often',value_map['often'])\n",
    "    temp_response=loneliness_values[i].split(',')\n",
    "    if('pre' in temp_response):\n",
    "        loneliness_responses_pre.append(temp_response)\n",
    "    elif('post' in temp_response):\n",
    "        loneliness_responses_post.append(temp_response)\n",
    "        \n",
    "print len(loneliness_responses_post)\n",
    "print len(loneliness_responses_pre)\n",
    "\n",
    "combined_scores=createAverageScores(loneliness_responses_pre,loneliness_responses_post)\n",
    "combined_scores\n",
    "fileWrite('data/Surveys/LonelinessScaleAVG.csv',combined_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u00', '2.0', '1.0', '0.5', '1.5', '2.5', '2.0', '0.0', '0.5', '2.5', '2.5', '1.0', '1.5', '0.0', '0.0', '2.5', '1.5', '2.5', '2.0', '3.0', '3.0']\n",
      "['u00', 'pre', '2', '1', '0', '0', '3', '2', '0', '0', '3', '3', '1', '1', '0', '0', '3', '1', '3', '2', '3', '3']\n",
      "['u00', 'post', '2', '1', '1', '3', '2', '2', '0', '1', '2', '2', '1', '2', '0', '0', '2', '2', '2', '2', '3', '3']\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print combined_scores[0]\n",
    "print loneliness_responses_pre[0]\n",
    "print loneliness_responses_post[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doing the same thing with the panas file. Read the values, change the strings into numbers. find average of pre and post values and\n",
    "#write into another file\n",
    "panas_values=open('data/Surveys/panas.csv').read().splitlines()\n",
    "panas_responses_pre=[]\n",
    "panas_responses_post=[]\n",
    "\n",
    "for i in range(len(panas_values)):\n",
    "    temp_response=panas_values[i].split(',')\n",
    "    if('pre' in temp_response):\n",
    "        panas_responses_pre.append(temp_response)\n",
    "    elif('post' in temp_response):\n",
    "        panas_responses_post.append(temp_response)\n",
    "\n",
    "combined_scores=createAverageScores(panas_responses_pre,panas_responses_post)\n",
    "\n",
    "fileWrite('data/Surveys/PanasScaleAVG.csv',combined_scores)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u00', '4.5', '3.5', '3.0', '3.5', '3.5', '3.0', '3.5', '3.0', '3.5', '3.5', '4.0', '3.5', '1.5', '3.5', '3.0', '2.5', '3.0', '3.0']\n",
      "['u00', 'pre', '5', '4', '3', '4', '3', '5', '5', '3', '3', '3', '4', '4', '', '4', '3', '2', '2', '5']\n",
      "['u00', 'post', '4', '3', '3', '3', '4', '1', '2', '3', '4', '4', '4', '3', '3', '3', '3', '3', '4', '1']\n"
     ]
    }
   ],
   "source": [
    "print combined_scores[0]\n",
    "print panas_responses_pre[0]\n",
    "print panas_responses_post[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "value_map= {'never': '0', 'almost never': '1','sometimes': '2', 'fairly often': '3','very often': '4'}\n",
    "stress_values=open('data/Surveys/PerceivedStressScale.csv').read().splitlines()\n",
    "stress_responses_pre=[]\n",
    "stress_responses_post=[]\n",
    "\n",
    "for i in range(len(stress_values)):\n",
    "    stress_values[i]=stress_values[i].replace('Never',value_map['never'])\n",
    "    stress_values[i]=stress_values[i].replace('Almost never',value_map['almost never'])\n",
    "    stress_values[i]=stress_values[i].replace('Sometime',value_map['sometimes'])\n",
    "    stress_values[i]=stress_values[i].replace('Fairly often',value_map['fairly often'])\n",
    "    stress_values[i]=stress_values[i].replace('Very often',value_map['very often'])\n",
    "\n",
    "    temp_response=stress_values[i].split(',')\n",
    "    if('pre' in temp_response):\n",
    "        stress_responses_pre.append(temp_response)\n",
    "    elif('post' in temp_response):\n",
    "        stress_responses_post.append(temp_response)\n",
    "print len(stress_responses_post)\n",
    "print len(stress_responses_pre)\n",
    "\n",
    "combined_scores=createAverageScores(stress_responses_pre,stress_responses_post)\n",
    "\n",
    "fileWrite('data/Surveys/StressScaleAVG.csv',combined_scores)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "value_map= {'not at all': '0', 'more than half': '1','several': '2', 'nearly everyday': '3'}\n",
    "difficulty_map={'not at all':'0','somewhat':'1','very':'2','extremely':'3'}\n",
    "phq_values=open('data/Surveys/PHQ-9.csv').read().splitlines()\n",
    "phq_responses_pre=[]\n",
    "phq_responses_post=[]\n",
    "\n",
    "for i in range(len(phq_values)):\n",
    "    phq_values[i]=phq_values[i].replace('Not at all',value_map['not at all'])\n",
    "    phq_values[i]=phq_values[i].replace('More than half the days',value_map['more than half'])\n",
    "    phq_values[i]=phq_values[i].replace('Several days',value_map['several'])\n",
    "    phq_values[i]=phq_values[i].replace('Nearly every day',value_map['nearly everyday'])\n",
    "    phq_values[i]=phq_values[i].replace('Not difficult at all',difficulty_map['not at all'])\n",
    "    phq_values[i]=phq_values[i].replace('Somewhat difficult',difficulty_map['somewhat'])\n",
    "    phq_values[i]=phq_values[i].replace('Very difficult',difficulty_map['very'])\n",
    "    phq_values[i]=phq_values[i].replace('Extremely difficult',difficulty_map['extremely'])\n",
    "\n",
    "    temp_response=phq_values[i].split(',')\n",
    "    if('pre' in temp_response):\n",
    "        phq_responses_pre.append(temp_response)\n",
    "    elif('post' in temp_response):\n",
    "        phq_responses_post.append(temp_response)\n",
    "print len(phq_responses_post)\n",
    "print len(phq_responses_pre)\n",
    "\n",
    "combined_scores=createAverageScores(phq_responses_pre,phq_responses_post)\n",
    "\n",
    "fileWrite('data/Surveys/PHQScaleAVG.csv',combined_scores)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u00', '0.0', '1.0', '0.0', '2.0', '0.0', '1.0', '1.0', '0.0', '0.0', '0.5']\n",
      "['u00', 'pre', '0', '2', '0', '2', '0', '0', '0', '0', '0', '0']\n",
      "['u00', 'post', '0', '0', '0', '2', '0', '2', '2', '0', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "print combined_scores[0]\n",
    "print phq_responses_pre[0]\n",
    "print phq_responses_post[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "phq_csv = pd.read_csv('data/Surveys/PHQScaleAVG.csv' , names = ['uid','a','b','c','d','e','f','g','h','i','j'])\n",
    "stress_scale_csv = pd.read_csv('data/Surveys/StressScaleAVG.csv', names = ['uid','a','b','c','d','e','f','g','h','i','j'])\n",
    "panas_scale_csv = pd.read_csv('data/Surveys/PanasScaleAVG.csv', names = ['uid','1','b2','c3','d4','e5','f6','g7','h8','i9','j10','11','12','13','14',\n",
    "                                                                         '15','16','17','18'])\n",
    "loneliness_scale_csv = pd.read_csv('data/Surveys/LonelinessScaleAVG.csv', names = ['uid','l1','l2','l3','l4','l5','l6','l7','l8','l9','l10','l11'\n",
    "                                                                                  ,'l12','l13','l14','l15','l16','l17','l18','l19','l20'])\n",
    "flourishing_scale_csv = pd.read_csv('data/Surveys/FlourishingScaleAVG.csv', names = ['uid','f1','f2','f3','f4','f5','f6','f7','f8'])\n",
    "\n",
    "\n",
    "\n",
    "result=pd.merge(phq_csv,stress_scale_csv,how='inner', on=['uid'])\n",
    "result.to_csv('resultFeatureVector.csv')\n",
    "\n",
    "result2=pd.merge(result,panas_scale_csv,how='inner',on=['uid'])\n",
    "result2.to_csv('result2FeatureVector.csv')\n",
    "\n",
    "result3=pd.merge(result2,loneliness_scale_csv,how='inner',on=['uid'])\n",
    "result3.to_csv('result3FeatureVector.csv')\n",
    "\n",
    "result4=pd.merge(result3,flourishing_scale_csv,how='inner',on=['uid'])\n",
    "result4.to_csv('SurveyResultsFeatureVector.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timeconvert(stampstr):\n",
    "    return datetime.fromtimestamp(long(stampstr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findDurations(startlist,endlist):\n",
    "    durationList=[]\n",
    "    durationList2=[]\n",
    "\n",
    "    for i in range(len(startlist)):\n",
    "        t1=long(startlist[i])\n",
    "        t2=long(endlist[i])\n",
    "        d=t2-t1\n",
    "        durationList2.append(int(d))        \n",
    "                \n",
    "    return durationList2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "import glob\n",
    "addList=[]\n",
    "\n",
    "filepaths=glob.glob(\"data/SensingData/Conversations/*.csv\")\n",
    "for filepath in filepaths:\n",
    "    \n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    conversation_csv = pd.read_csv(filepath , names = ['start','stop'])\n",
    "    startConvos=conversation_csv.start.tolist()[1:]\n",
    "    endConvos=conversation_csv.stop.tolist()[1:]\n",
    "\n",
    "    #the array of durations gives the durations of conversations for one user \n",
    "    durations=findDurations(startConvos,endConvos) \n",
    "\n",
    "    #find its lenght, mean and sd\n",
    "    mean=statistics.mean(durations)\n",
    "    sdv=statistics.stdev(durations)\n",
    "    addList.append([uid, len(durations),mean, sdv])\n",
    "\n",
    "fileWrite('durationOfConversations.csv',addList)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepaths=glob.glob(\"data/SensingData/CallLog/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wholeList=[]\n",
    "for filepath in filepaths:\n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    callLog_csv = pd.read_csv(filepath , names = ['id','device','timestamp','1','2','3','4','5','6','7','8'])\n",
    "    callList=[]\n",
    "    callList=callLog_csv.id.tolist()\n",
    "    callList=callList[1:]\n",
    "    myset = set(callList) \n",
    "    numberOfCalls=len(myset)\n",
    "    wholeList.append([uid, numberOfCalls])\n",
    "fileWrite('numberOfCalls.csv',wholeList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SensingData/Wifi_Location\\wifi_location_u00.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u01.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u02.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u03.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u04.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u05.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u07.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u08.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u09.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u10.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u12.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u13.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u14.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u15.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u16.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u17.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u18.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u19.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u20.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u22.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u23.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u24.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u25.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u27.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u30.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u31.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u32.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u33.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u34.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u35.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u36.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u39.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u41.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u42.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u43.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u44.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u45.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u46.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u47.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u49.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u50.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u51.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u52.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u53.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u54.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u56.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u57.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u58.csv\n",
      "data/SensingData/Wifi_Location\\wifi_location_u59.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2902: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# getting mobility data.\n",
    "#total number of locations collected\n",
    "#total number of unique locations \n",
    "filepaths=glob.glob(\"data/SensingData/Wifi_Location/*.csv\")\n",
    "wholeList=[]\n",
    "for filepath in filepaths:\n",
    "    print filepath\n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    wifi_csv = pd.read_csv(filepath , names = ['time','location'])\n",
    "    \n",
    "    wifi_time_List=wifi_csv.time.tolist()[1:]\n",
    "    wifi_location_List=wifi_csv.location.tolist()[1:]\n",
    "  \n",
    "    totalLocationsCollected=len(wifi_location_List)\n",
    "    uniqueLocationsCollected=len(set(wifi_location_List))\n",
    "\n",
    "    wholeList.append([uid,totalLocationsCollected,uniqueLocationsCollected])\n",
    "    \n",
    "fileWrite('WifiLocationsFeatureVectors.csv',wholeList) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SensingData/PhysicalActivity\\activity_u00.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u01.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u02.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u03.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u04.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u05.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u07.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u08.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u09.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u10.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u12.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u13.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u14.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u15.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u16.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u17.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u18.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u19.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u20.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u22.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u23.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u24.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u25.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u27.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u30.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u31.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u32.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u33.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u34.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u35.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u36.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u39.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u41.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u42.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u43.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u44.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u45.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u46.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u47.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u49.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u50.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u51.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u52.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u53.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u54.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u56.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u57.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u58.csv\n",
      "0\n",
      "data/SensingData/PhysicalActivity\\activity_u59.csv\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2902: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#activity feature vector\n",
    "#mode (most frequent) activity\n",
    "#and the proportion of activity that is running/walking spanning the entire period of the study\n",
    "#0 = Stationary; 1 = Walking; 2 = Running; = Unknown\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "filepaths=glob.glob(\"data/SensingData/PhysicalActivity/*.csv\")\n",
    "wholeList=[]\n",
    "for filepath in filepaths:\n",
    "    print filepath\n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    activity_csv = pd.read_csv(filepath , names = ['time','inference'])\n",
    "    activity_time_List=activity_csv.time.tolist()[1:]\n",
    "    activity_type_List=activity_csv.inference.tolist()[1:]\n",
    "    #find mode of the activity type list.\n",
    "    \n",
    "    #print activity_type_List\n",
    "    data = Counter(activity_type_List)\n",
    "#     print data.most_common()   # Returns all unique items and their counts\n",
    "#     print data.most_common(1)  # Returns the highest occurring item\n",
    "    mode=statistics.mode(activity_type_List)\n",
    "    print mode\n",
    "    #find the counts of 1 nd 2s in the list. \n",
    "    walkingNumber=0\n",
    "    runningNumber=0\n",
    "    c=0.0\n",
    "    \n",
    "    for item in activity_type_List:\n",
    "        if item == '1' or item =='2':\n",
    "            c+=1.0\n",
    "        \n",
    "    proportion_of_walking_running=float(c)/float(len(activity_type_List))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    tempList=[]\n",
    "    tempList.append(uid)\n",
    "    tempList.append(str(mode))\n",
    "    tempList.append(proportion_of_walking_running)\n",
    "    wholeList.append(tempList)\n",
    "    \n",
    "fileWrite('physicalActivityFeatures.csv',wholeList) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['u00', '0', 0.04660480398968811],\n",
       " ['u01', '0', 0.03841747707755095],\n",
       " ['u02', '0', 0.027214237239297997],\n",
       " ['u03', '0', 0.009093295608674726],\n",
       " ['u04', '0', 0.06664850513279541],\n",
       " ['u05', '0', 0.023716889962196303],\n",
       " ['u07', '0', 0.03887210836666529],\n",
       " ['u08', '0', 0.03221006323677129],\n",
       " ['u09', '0', 0.03469898128057078],\n",
       " ['u10', '0', 0.018228063536530493],\n",
       " ['u12', '0', 0.01695734681952198],\n",
       " ['u13', '0', 0.01669439948941214],\n",
       " ['u14', '0', 0.06425855934739479],\n",
       " ['u15', '0', 0.03614536226697466],\n",
       " ['u16', '0', 0.025222669837330285],\n",
       " ['u17', '0', 0.018799121326826894],\n",
       " ['u18', '0', 0.022833591721949813],\n",
       " ['u19', '0', 0.020545299922567487],\n",
       " ['u20', '0', 0.020488540307963342],\n",
       " ['u22', '0', 0.030678182835871835],\n",
       " ['u23', '0', 0.013131766062391904],\n",
       " ['u24', '0', 0.053329917827738445],\n",
       " ['u25', '0', 0.06869590087399793],\n",
       " ['u27', '0', 0.016897207997331607],\n",
       " ['u30', '0', 0.040277258535501015],\n",
       " ['u31', '0', 0.021655053281980766],\n",
       " ['u32', '0', 0.033661277056414356],\n",
       " ['u33', '0', 0.02944415559861984],\n",
       " ['u34', '0', 0.03199009324812938],\n",
       " ['u35', '0', 0.022470652536855713],\n",
       " ['u36', '0', 0.030127255562633837],\n",
       " ['u39', '0', 0.0033509323404644917],\n",
       " ['u41', '0', 0.02462575558249714],\n",
       " ['u42', '0', 0.021015452050554266],\n",
       " ['u43', '0', 0.04353007009777716],\n",
       " ['u44', '0', 0.012005577878371735],\n",
       " ['u45', '0', 0.017270618075285838],\n",
       " ['u46', '0', 0.06544495935506478],\n",
       " ['u47', '0', 0.012242917585304476],\n",
       " ['u49', '0', 0.02091421987430252],\n",
       " ['u50', '0', 0.015000735845486463],\n",
       " ['u51', '0', 0.033312016418981474],\n",
       " ['u52', '0', 0.16925714711692572],\n",
       " ['u53', '0', 0.014553968959358728],\n",
       " ['u54', '0', 0.05122556268446816],\n",
       " ['u56', '0', 0.02717490813245958],\n",
       " ['u57', '0', 0.04179647647005938],\n",
       " ['u58', '0', 0.018528734268568773],\n",
       " ['u59', '0', 0.012506637565045567]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SensingData/PhoneLight\\dark_u00.csv\n",
      "mean :  9595.92391304\n",
      "data/SensingData/PhoneLight\\dark_u01.csv\n",
      "mean :  12134.0682927\n",
      "data/SensingData/PhoneLight\\dark_u02.csv\n",
      "mean :  11389.0695187\n",
      "data/SensingData/PhoneLight\\dark_u03.csv\n",
      "mean :  14019.7301587\n",
      "data/SensingData/PhoneLight\\dark_u04.csv\n",
      "mean :  11836.7126866\n",
      "data/SensingData/PhoneLight\\dark_u05.csv\n",
      "mean :  13043.0882353\n",
      "data/SensingData/PhoneLight\\dark_u07.csv\n",
      "mean :  13653.8092105\n",
      "data/SensingData/PhoneLight\\dark_u08.csv\n",
      "mean :  10381.009434\n",
      "data/SensingData/PhoneLight\\dark_u09.csv\n",
      "mean :  11098.7539267\n",
      "data/SensingData/PhoneLight\\dark_u10.csv\n",
      "mean :  11093.2442396\n",
      "data/SensingData/PhoneLight\\dark_u12.csv\n",
      "mean :  10734.401487\n",
      "data/SensingData/PhoneLight\\dark_u13.csv\n",
      "mean :  14594.8157895\n",
      "data/SensingData/PhoneLight\\dark_u14.csv\n",
      "mean :  13691.3265306\n",
      "data/SensingData/PhoneLight\\dark_u15.csv\n",
      "mean :  14333.2857143\n",
      "data/SensingData/PhoneLight\\dark_u16.csv\n",
      "mean :  10564.4355828\n",
      "data/SensingData/PhoneLight\\dark_u17.csv\n",
      "mean :  12080.7099567\n",
      "data/SensingData/PhoneLight\\dark_u18.csv\n",
      "mean :  11817.92\n",
      "data/SensingData/PhoneLight\\dark_u19.csv\n",
      "mean :  13752.3296089\n",
      "data/SensingData/PhoneLight\\dark_u20.csv\n",
      "mean :  12465.3736264\n",
      "data/SensingData/PhoneLight\\dark_u22.csv\n",
      "mean :  11872.2268041\n",
      "data/SensingData/PhoneLight\\dark_u23.csv\n",
      "mean :  13616.7853403\n",
      "data/SensingData/PhoneLight\\dark_u24.csv\n",
      "mean :  9906.14285714\n",
      "data/SensingData/PhoneLight\\dark_u25.csv\n",
      "mean :  9456.25773196\n",
      "data/SensingData/PhoneLight\\dark_u27.csv\n",
      "mean :  9446.00961538\n",
      "data/SensingData/PhoneLight\\dark_u30.csv\n",
      "mean :  12392.3298429\n",
      "data/SensingData/PhoneLight\\dark_u31.csv\n",
      "mean :  9958.11\n",
      "data/SensingData/PhoneLight\\dark_u32.csv\n",
      "mean :  12715.1206897\n",
      "data/SensingData/PhoneLight\\dark_u33.csv\n",
      "mean :  12645.3555556\n",
      "data/SensingData/PhoneLight\\dark_u34.csv\n",
      "mean :  11209.5294118\n",
      "data/SensingData/PhoneLight\\dark_u35.csv\n",
      "mean :  12580.6170213\n",
      "data/SensingData/PhoneLight\\dark_u36.csv\n",
      "mean :  10981.0286885\n",
      "data/SensingData/PhoneLight\\dark_u39.csv\n",
      "mean :  17440.75\n",
      "data/SensingData/PhoneLight\\dark_u41.csv\n",
      "mean :  11516.9315068\n",
      "data/SensingData/PhoneLight\\dark_u42.csv\n",
      "mean :  14530.7708333\n",
      "data/SensingData/PhoneLight\\dark_u43.csv\n",
      "mean :  12297.057554\n",
      "data/SensingData/PhoneLight\\dark_u44.csv\n",
      "mean :  12096.1904762\n",
      "data/SensingData/PhoneLight\\dark_u45.csv\n",
      "mean :  14543.4782609\n",
      "data/SensingData/PhoneLight\\dark_u46.csv\n",
      "mean :  10053.7682927\n",
      "data/SensingData/PhoneLight\\dark_u47.csv\n",
      "mean :  13019.7391304\n",
      "data/SensingData/PhoneLight\\dark_u49.csv\n",
      "mean :  10203.9104478\n",
      "data/SensingData/PhoneLight\\dark_u50.csv\n",
      "mean :  13562.1129032\n",
      "data/SensingData/PhoneLight\\dark_u51.csv\n",
      "mean :  10901.8798077\n",
      "data/SensingData/PhoneLight\\dark_u52.csv\n",
      "mean :  10474.9330144\n",
      "data/SensingData/PhoneLight\\dark_u53.csv\n",
      "mean :  13498.8790323\n",
      "data/SensingData/PhoneLight\\dark_u54.csv\n",
      "mean :  9892.39072848\n",
      "data/SensingData/PhoneLight\\dark_u56.csv\n",
      "mean :  12912.8878505\n",
      "data/SensingData/PhoneLight\\dark_u57.csv\n",
      "mean :  11029.4034335\n",
      "data/SensingData/PhoneLight\\dark_u58.csv\n",
      "mean :  11290.0913978\n",
      "data/SensingData/PhoneLight\\dark_u59.csv\n",
      "mean :  10667.4\n"
     ]
    }
   ],
   "source": [
    "#“PhoneLight” and “PhoneLock”\n",
    "#feature vector of phone (non)-use: mean duration when phone was in a dark environment; \n",
    "#standard deviation of the duration when phone was in a dark environment; \n",
    "#mean phone lock duration; \n",
    "#and the standard deviation of phone lock duration\n",
    "\n",
    "\n",
    "filepaths=glob.glob(\"data/SensingData/PhoneLight/*.csv\")\n",
    "wholeList=[]\n",
    "for filepath in filepaths:\n",
    "    print filepath\n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    phonedark_csv = pd.read_csv(filepath , names = ['start','end'])\n",
    "    phonedark_start_List=[]\n",
    "    phonedark_start_List=phonedark_csv.start.tolist()[1:]\n",
    "    phonedark_end_List=[]\n",
    "    phonedark_end_List=phonedark_csv.end.tolist()[1:]   \n",
    "    durations_dark=findDurations(phonedark_start_List,phonedark_end_List)\n",
    "    durations_int=[]\n",
    "    for dur in durations_dark:\n",
    "        durations_int.append(int(dur))\n",
    "   \n",
    "    mean_dark=statistics.mean(durations_int)    \n",
    "    print \"mean : \", mean_dark\n",
    "    sd_dark=statistics.stdev(durations_int)    \n",
    "    tempList=[]\n",
    "    tempList.append(uid)\n",
    "    tempList.append(mean_dark)\n",
    "    tempList.append(sd_dark)\n",
    "    wholeList.append(tempList)\n",
    "    \n",
    "fileWrite('phoneDarkFeatures.csv',wholeList) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filetype2ToX(filepath):\n",
    "    X=[]\n",
    "    Y=[]\n",
    " \n",
    "    fileList=open(filepath).read().splitlines()\n",
    "    fileList=fileList[2:]\n",
    "    \n",
    "    \n",
    "    for line in fileList:\n",
    "        temp=line.split(',')\n",
    "        temp2=[]\n",
    "        if(temp[1] in dictGrades.keys()):\n",
    "            for x in temp[2:]:\n",
    "                \n",
    "                temp2.append(float(x))\n",
    "                        \n",
    "            X.append(temp2[])\n",
    "            Y.append(dictGrades[temp[1]])\n",
    "        \n",
    "            \n",
    "    no_of_observations = len(X)\n",
    "    no_of_features = len(X[0])\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X_ = StandardScaler().fit_transform(X) \n",
    "    X_pca = PCA(n_components=10).fit_transform(X_)\n",
    "    \n",
    "    model = sm.OLS(Y, X_pca)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  9457.80530973\n",
      "mean :  14280.3564815\n",
      "mean :  10340.1089744\n",
      "mean :  14850.0392157\n",
      "mean :  14496.5518672\n",
      "mean :  13357.5444444\n",
      "mean :  15476.0718954\n",
      "mean :  8540.22463768\n",
      "mean :  14803.7442922\n",
      "mean :  9943.0166113\n",
      "mean :  9373.5339233\n",
      "mean :  10137.3401015\n",
      "mean :  11295.1382488\n",
      "mean :  15342.6896552\n",
      "mean :  10656.0371901\n",
      "mean :  13126.231441\n",
      "mean :  12377.0\n",
      "mean :  11370.4724409\n",
      "mean :  12518.5363636\n",
      "mean :  11841.6132075\n",
      "mean :  10393.1635514\n",
      "mean :  11099.8492063\n",
      "mean :  12007.9807692\n",
      "mean :  11037.6122449\n",
      "mean :  13422.5720339\n",
      "mean :  7803.51442308\n",
      "mean :  12776.622807\n",
      "mean :  10328.3064516\n",
      "mean :  13038.45\n",
      "mean :  12577.9711538\n",
      "mean :  9684.88288288\n",
      "mean :  10640.6923077\n",
      "mean :  14247.6617647\n",
      "mean :  12092.7653061\n",
      "mean :  12356.6790123\n",
      "mean :  11237.7955556\n",
      "mean :  10126.0706522\n",
      "mean :  9354.07317073\n",
      "mean :  11501.1058824\n",
      "mean :  8215.4265233\n",
      "mean :  11658.3762376\n",
      "mean :  12287.4651899\n",
      "mean :  9335.05063291\n",
      "mean :  12065.8962656\n",
      "mean :  8722.34545455\n",
      "mean :  13267.3941606\n",
      "mean :  8686.99657534\n",
      "mean :  12035.7443609\n",
      "mean :  10468.0896552\n"
     ]
    }
   ],
   "source": [
    "filepaths=glob.glob(\"data/SensingData/PhoneLock/*.csv\")\n",
    "wholeList=[]\n",
    "for filepath in filepaths:\n",
    "    i=filepath.rindex('u')\n",
    "    e=filepath.rindex('.')\n",
    "    uid=filepath[i:e]\n",
    "    phonelock_csv = pd.read_csv(filepath , names = ['start','end'])\n",
    "    phonelock_start_List=[]\n",
    "    phonelock_start_List=phonelock_csv.start.tolist()[1:]\n",
    "    phonelock_end_List=[]\n",
    "    phonelock_end_List=phonelock_csv.end.tolist()[1:]   \n",
    "    durations_lock=findDurations(phonelock_start_List,phonelock_end_List)\n",
    "    durations_int=[]\n",
    "    for dur in durations_lock:\n",
    "        durations_int.append(int(dur))\n",
    "   \n",
    "    mean_lock=statistics.mean(durations_int)    \n",
    "    print \"mean : \", mean_lock\n",
    "    sd_lock=statistics.stdev(durations_int)    \n",
    "    tempList=[]\n",
    "    tempList.append(uid)\n",
    "    tempList.append(mean_lock)\n",
    "    tempList.append(sd_lock)\n",
    "    wholeList.append(tempList)\n",
    "    \n",
    "fileWrite('phoneLOCKFeatures.csv',wholeList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    uid  number_x  number_y         mean           sd\n",
      "0   uid         1         1     2.000000     3.000000\n",
      "1   u00      1759      2111   760.111795  1158.037964\n",
      "2   u01      1486      2179   764.026159  1031.949763\n",
      "3   u02      1631      1483   849.729602  1269.353540\n",
      "4   u03      1269       478   519.866109   819.854491\n",
      "5   u04      1443      2737   550.232371   714.199521\n",
      "6   u05      1455      1640   662.570732  1030.285297\n",
      "7   u07      1217      1373   588.976693   958.204897\n",
      "8   u08      1575      2560   588.649609   821.954144\n",
      "9   u09      1564      2269   755.657118  1066.206930\n",
      "10  u10      1662      2260   510.745133   794.556541\n",
      "11  u12      1684      1796   720.531737  1019.364039\n",
      "12  u13      1435      1915   418.838642   636.733026\n",
      "13  u14      1400      1879   871.337946  1298.966236\n",
      "14  u15       806      1052   734.803232   918.054582\n",
      "15  u16      1505      1723   596.967499   881.176696\n",
      "16  u17      1613      1396   626.042264   947.014013\n",
      "17  u18      1335      1051   739.227402  1034.116681\n",
      "18  u19      1624      1686   620.862989  1074.985033\n",
      "19  u20      1107      1052   451.978137   766.787638\n",
      "20  u22      1390      1109   585.124436   910.979502\n",
      "21  u23      1408      1315   472.834981   697.016981\n",
      "22  u24       801      1048   734.900763  1048.176593\n",
      "23  u25       781      1121   865.824264  1418.205992\n",
      "24  u27      1610      2212   459.150995   734.202034\n",
      "25  u30      1529      2107   917.590888  1298.922304\n",
      "26  u31      1440      2555   463.342074   661.702641\n",
      "27  u32      1241      1955   836.494629  1245.102629\n",
      "28  u33      1292       902  1051.056541  1510.191136\n",
      "29  u34       828      1139   612.045654   813.396964\n",
      "30  u35      1612      1427   537.219341   764.358489\n",
      "31  u36      1815      1914   418.650993   661.708437\n",
      "32  u39       534       435   255.691954   279.313117\n",
      "33  u41      1100      1213   664.974444   922.522464\n",
      "34  u42      1082      1147   601.419355   804.526320\n",
      "35  u43      1246      1731   602.821490  1095.757132\n",
      "36  u44      1466      1401   456.995717   641.068990\n",
      "37  u45       945      1259   632.503574   917.013002\n",
      "38  u46      1344      1907   754.106974  1219.746965\n",
      "39  u47       973       902   400.430155   613.966617\n",
      "40  u49      1541      1662   525.607100   867.437482\n",
      "41  u50       953       927   412.362460   606.239378\n",
      "42  u51      2156      1936   452.856921   731.598139\n",
      "43  u52      1588       778   460.849614   768.912791\n",
      "44  u53      1472      1837   563.770822   934.523517\n",
      "45  u54      1047      1269   626.647754   885.269137\n",
      "46  u56      1552      1424   513.384831   795.660037\n",
      "47  u57      1625      2249   653.361939  1066.176544\n",
      "48  u58      1514      1417   846.889203  1368.921315\n",
      "49  u59      1745      4085   393.540514   839.099075\n"
     ]
    }
   ],
   "source": [
    "#to create the social engagement feature vectors, read the files \"numberOfcalls.csv\" and \"durationOfConversations.csv\"\n",
    "#combine the files based on UID to get the final feature vectors. \n",
    "\n",
    "calls_csv = pd.read_csv('numberOfCalls.csv' , names = ['uid','number'])\n",
    "#print calls_csv\n",
    "conversation_duration_csv= pd.read_csv('durationOfConversations.csv', names = ['uid','number','mean','sd'])\n",
    "\n",
    "result=pd.merge(calls_csv,conversation_duration_csv,how='inner', on=['uid'])\n",
    "print result\n",
    "result.to_csv('EngagementFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    uid        mean_x          sd_x        mean_y          sd_y\n",
      "0   uid      1.000000      2.000000      1.000000      2.000000\n",
      "1   u00   9595.923913   7795.829294   9457.805310   7781.888413\n",
      "2   u01  12134.068293   8802.510971  14280.356481   9809.882005\n",
      "3   u02  11389.069519   9904.063334  10340.108974   8436.168304\n",
      "4   u03  14019.730159   9848.935253  14850.039216  10342.933501\n",
      "5   u04  11836.712687   8184.961280  14496.551867   8623.549686\n",
      "6   u05  13043.088235  10219.703434  13357.544444  10195.234540\n",
      "7   u07  13653.809210  10257.780301  15476.071895  10464.411037\n",
      "8   u08  10381.009434   7798.399678   8540.224638   5830.094882\n",
      "9   u09  11098.753927   8685.055793  14803.744292  10322.248829\n",
      "10  u10  11093.244240   9051.514723   9943.016611   8173.104726\n",
      "11  u12  10734.401487   9580.379222   9373.533923   7594.979017\n",
      "12  u13  14594.815790  11020.922615  10137.340101   7969.248777\n",
      "13  u14  13691.326531   9799.055863  11295.138249   8537.862614\n",
      "14  u15  14333.285714  10267.859849  15342.689655  11411.236136\n",
      "15  u16  10564.435583   8393.981715  10656.037190   8475.263103\n",
      "16  u17  12080.709957   8492.383617  13126.231441   8984.567071\n",
      "17  u18  11817.920000   9505.419461  12377.000000   9095.261655\n",
      "18  u19  13752.329609  10767.017908  11370.472441   9092.576242\n",
      "19  u20  12465.373626   8813.759147  12518.536364   9025.403511\n",
      "20  u22  11872.226804   8869.004609  11841.613208   9265.149255\n",
      "21  u23  13616.785340  10137.979650  10393.163551   7426.314068\n",
      "22  u24   9906.142857   8414.857545  11099.849206   8525.301545\n",
      "23  u25   9456.257732   6567.428600  12007.980769   8578.286131\n",
      "24  u27   9446.009615   7730.608074  11037.612245   8299.573638\n",
      "25  u30  12392.329843   9890.919259  13422.572034   9659.828725\n",
      "26  u31   9958.110000   7325.522361   7803.514423   4737.325738\n",
      "27  u32  12715.120690   9798.602495  12776.622807   9563.963103\n",
      "28  u33  12645.355556   9467.116287  10328.306452   8151.504400\n",
      "29  u34  11209.529412   7327.137532  13038.450000   8272.311322\n",
      "30  u35  12580.617021   7991.451878  12577.971154   8431.686813\n",
      "31  u36  10981.028689   9468.848019   9684.882883   8181.562712\n",
      "32  u39  17440.750000  12260.413775  10640.692308   8954.936389\n",
      "33  u41  11516.931507   9135.925648  14247.661765   9905.838697\n",
      "34  u42  14530.770833   9420.365923  12092.765306   8435.109734\n",
      "35  u43  12297.057554   9949.044122  12356.679012   9549.971546\n",
      "36  u44  12096.190476   9544.687418  11237.795556   8998.872656\n",
      "37  u45  14543.478261  10194.237410  10126.070652   6227.654019\n",
      "38  u46  10053.768293   7182.288357   9354.073171   7381.047687\n",
      "39  u47  13019.739130   9356.503432  11501.105882   8038.985227\n",
      "40  u49  10203.910448   7259.371065   8215.426523   5418.767490\n",
      "41  u50  13562.112903  10495.131727  11658.376238   7959.471734\n",
      "42  u51  10901.879808   8417.887140  12287.465190   9106.692094\n",
      "43  u52  10474.933014   7767.538273   9335.050633   7037.737479\n",
      "44  u53  13498.879032   9356.517608  12065.896266   7556.852143\n",
      "45  u54   9892.390728   7465.103804   8722.345455   6200.315035\n",
      "46  u56  12912.887850   9922.040842  13267.394161   9897.752128\n",
      "47  u57  11029.403433   9037.998632   8686.996575   5898.607675\n",
      "48  u58  11290.091398   8822.349996  12035.744361   8576.333373\n",
      "49  u59  10667.400000   7903.457301  10468.089655   7487.705236\n"
     ]
    }
   ],
   "source": [
    "#to create the  phone non use feature vectors, read the files \"phoneLOCKFeatures.csv\" and \"phoneDarkFeatures.csv\"\n",
    "#combine the files based on UID to get the final feature vectors. \n",
    "\n",
    "phoneDark_csv = pd.read_csv('phonedarkFeatures.csv' , names = ['uid','mean','sd'])\n",
    "\n",
    "phoneLock_csv= pd.read_csv('phoneLOCKFeatures.csv', names = ['uid','mean','sd'])\n",
    "\n",
    "result=pd.merge(phoneDark_csv,phoneLock_csv,how='right', on=['uid'])\n",
    "print result\n",
    "result.to_csv('PhoneNonUseFeatures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u01,2.863,1.777777778,3\n",
      "u02,3.505,4,4\n",
      "u04,3.029,3.5,4\n",
      "u05,3.679,3.777777778,4\n",
      "u07,3.474,4,4\n",
      "u08,3.705,3.333333333,4\n",
      "u09,3.806,3.777777778,3.666666667\n",
      "u10,3.667,3.777777778,4\n",
      "u12,3.245,2.583333333,2.333333333\n",
      "u14,3.293,3.888888889,3.666666667\n",
      "u15,2.815,3.333333333,3.666666667\n",
      "u16,3.373,4,4\n",
      "u17,3.476,3.333333333,4\n",
      "u18,3.474,3.333333333,3.333333333\n",
      "u19,3.947,3.777777778,3.333333333\n",
      "u22,3.889,3.916666667,4\n",
      "u24,2.987,3.222222222,3.333333333\n",
      "u25,2.765,3.333333333,3.666666667\n",
      "u27,3.719,3.333333333,3\n",
      "u30,3.93,3.916666667,4\n",
      "u32,3.826,3.888888889,4\n",
      "u33,2.815,2.777777778,4\n",
      "u41,3.652,3.777777778,4\n",
      "u43,3.79,4,4\n",
      "u46,3.646,1.111111111,4\n",
      "u49,3.625,3.777777778,4\n",
      "u52,2.4,1,0\n",
      "u54,3.343,3.444444444,4\n",
      "u57,3.389,2.666666667,4\n",
      "u59,3.519,3.555555556,3.666666667\n",
      "{'u43': 3.79, 'u19': 3.947, 'u18': 3.474, 'u41': 3.652, 'u33': 2.815, 'u32': 3.826, 'u30': 3.93, 'u10': 3.667, 'u12': 3.245, 'u15': 2.815, 'u14': 3.293, 'u17': 3.476, 'u16': 3.373, 'u59': 3.519, 'u08': 3.705, 'u54': 3.343, 'u57': 3.389, 'u52': 2.4, 'u49': 3.625, 'u24': 2.987, 'u25': 2.765, 'u27': 3.719, 'u46': 3.646, 'u22': 3.889, 'u09': 3.806, 'u07': 3.474, 'u04': 3.029, 'u05': 3.679, 'u02': 3.505, 'u01': 2.863}\n"
     ]
    }
   ],
   "source": [
    "WifiFeatureList=open('FeatureVectorFiles/WifiLocationsFeaturevectors.csv').read().splitlines()\n",
    "WifiFeatureList=WifiFeatureList[1:]\n",
    "dictGrades={}\n",
    "\n",
    "groundTruthList=open('data/GroundTruth/grades.csv').read().splitlines()\n",
    "groundTruthList=groundTruthList[1:]\n",
    "Y2=[]\n",
    "for line in groundTruthList:\n",
    "    print line\n",
    "    temp=line.split(',')[0:2]\n",
    "    temp2=[]\n",
    "    dictGrades[temp[0]]=float(temp[1])\n",
    "print dictGrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.921\n",
      "Model:                            OLS   Adj. R-squared:                  0.915\n",
      "Method:                 Least Squares   F-statistic:                     163.3\n",
      "Date:                Tue, 03 May 2016   Prob (F-statistic):           3.65e-16\n",
      "Time:                        13:44:13   Log-Likelihood:                -41.580\n",
      "No. Observations:                  30   AIC:                             87.16\n",
      "Df Residuals:                      28   BIC:                             89.96\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1          6.362e-07   2.82e-06      0.226      0.823     -5.14e-06  6.41e-06\n",
      "x2             0.0029      0.000     14.267      0.000         0.002     0.003\n",
      "==============================================================================\n",
      "Omnibus:                       12.802   Durbin-Watson:                   1.350\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               12.909\n",
      "Skew:                          -1.217   Prob(JB):                      0.00157\n",
      "Kurtosis:                       5.098   Cond. No.                         90.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def fileToX(filepath):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "   \n",
    " \n",
    "    fileList=open(filepath).read().splitlines()[1:]\n",
    "    for line in fileList:\n",
    "        temp=line.split(',')\n",
    "        temp2=[]\n",
    "        if(temp[0] in dictGrades.keys()):\n",
    "            for x in temp[1:-1]:\n",
    "                temp2.append(float(x))\n",
    "            X.append(temp2)\n",
    "            Y.append(dictGrades[temp[0]])\n",
    "    \n",
    "    model = sm.OLS(Y, X)\n",
    "    results = model.fit()\n",
    "    print(results.summary())\n",
    "\n",
    "fileToX('FeatureVectorFiles/WifiLocationsFeaturevectors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.584\n",
      "Model:                            OLS   Adj. R-squared:                  0.569\n",
      "Method:                 Least Squares   F-statistic:                     40.67\n",
      "Date:                Tue, 03 May 2016   Prob (F-statistic):           5.69e-07\n",
      "Time:                        13:44:13   Log-Likelihood:                -66.519\n",
      "No. Observations:                  30   AIC:                             135.0\n",
      "Df Residuals:                      29   BIC:                             136.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const               0          0        nan        nan             0         0\n",
      "x1            53.7933      8.436      6.377      0.000        36.541    71.046\n",
      "==============================================================================\n",
      "Omnibus:                       42.527   Durbin-Watson:                   1.354\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              167.576\n",
      "Skew:                          -2.890   Prob(JB):                     4.09e-37\n",
      "Kurtosis:                      13.032   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "fileToX('FeatureVectorFiles/physicalActivityFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filetype2ToX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-3ef2ac687865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiletype2ToX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FeatureVectorFiles/EngagementFeatures.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'filetype2ToX' is not defined"
     ]
    }
   ],
   "source": [
    "filetype2ToX('FeatureVectorFiles/EngagementFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX('FeatureVectorFiles/PhoneNonUseFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filetype2ToX_withPCA(filepath):\n",
    "    X=[]\n",
    "    Y=[]\n",
    " \n",
    "    fileList=open(filepath).read().splitlines()\n",
    "    fileList=fileList[2:]\n",
    "    \n",
    "    \n",
    "    for line in fileList:\n",
    "        temp=line.split(',')\n",
    "        temp2=[]\n",
    "        if(temp[1] in dictGrades.keys()):\n",
    "            for x in temp[2:]:\n",
    "                \n",
    "                temp2.append(float(x))\n",
    "                        \n",
    "            X.append(temp2[])\n",
    "            Y.append(dictGrades[temp[1]])\n",
    "        \n",
    "            \n",
    "    no_of_observations = len(X)\n",
    "    no_of_features = len(X[0])\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X_ = StandardScaler().fit_transform(X) \n",
    "    X_pca = PCA(n_components=10).fit_transform(X_)\n",
    "    \n",
    "    model = sm.OLS(Y, X_pca)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filetype2ToX_reduced_features(filepath,no_of_features=10):\n",
    "    X=[]\n",
    "    Y=[]\n",
    " \n",
    "    fileList=open(filepath).read().splitlines()\n",
    "    fileList=fileList[2:]\n",
    "    \n",
    "    \n",
    "    for line in fileList:\n",
    "        temp=line.split(',')\n",
    "        temp2=[]\n",
    "        if(temp[1] in dictGrades.keys()):\n",
    "            for x in temp[2:]:\n",
    "                \n",
    "                temp2.append(float(x))\n",
    "                        \n",
    "            X.append(temp2[:no_of_features])\n",
    "            Y.append(dictGrades[temp[1]])\n",
    "    \n",
    "    model = sm.OLS(Y, X)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX('FeatureVectorFiles/SurveyResultsFeatureVector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX_withPCA('FeatureVectorFiles/SurveyResultsFeatureVector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX_reduced_features('FeatureVectorFiles/SurveyResultsFeatureVector.csv', 10)\n",
    "# with phq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX_reduced_features('FeatureVectorFiles/SurveyResultsFeatureVector.csv',20)\n",
    "# with stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX_reduced_features('FeatureVectorFiles/SurveyResultsFeatureVector.csv',38)\n",
    "# panas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filetype2ToX_reduced_features('FeatureVectorFiles/SurveyResultsFeatureVector.csv',57)\n",
    "# loniness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the performance of the above model fits based on the Adj. R-squared metric given by the summary() function.  indicates the model fit for the particular fature category was the poorest, 1 indicates that it perfectly fit the grade data, and avalue between 0 and 1 indicate everything in between. . If lower values are better, then 0 should be the best fit?\n",
    "\n",
    "2) Discuss your rationale behind why one model( one feature category type) performs better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print the R Squared and Adjusted R-Squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "table = ListTable()\n",
    "table.append(['Feature Type','','R. Squared', 'Adjusted R-Squared'])\n",
    "table.append(['','','',''])\n",
    "table.append(['Wifi Location','','0.921','0.915'])\n",
    "table.append(['Physical Activity','','0.584','0.569'])\n",
    "table.append(['Engagement','','0.982','0.980'])\n",
    "table.append(['Phone Non Use','','0.980','0.977'])\n",
    "table.append(['','','',''])\n",
    "table.append(['Survey Results','','1.000','nan'])\n",
    "table.append(['Survey Results with PCA','','0.006','-0.579'])\n",
    "table.append(['Survey Results with phq','','0.773','0.640'])\n",
    "table.append(['Survey Results with phq & stress','','0.998','0.994'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analysing Adjusted R Squared Values\n",
    "\n",
    "    The table below shows the R Squared and Adjusted R Squared values for the different feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "#### Purpose of Adjusted R-Squared\n",
    "    Both the R-Squared and Adjusted R-Squared values are measures of model performance. Possible values range from 0.0 to 1.0. The Adjusted R-Squared value is always a bit lower than the R-Squared value because it reflects model complexity (the number of variables) as it relates to the data, and consequently is a more accurate measure of model performance. It adjusts the statistic based on the number of independent variables in the model. Also, adding an additional explanatory variable to the model will likely increase the R-Squared value, but decrease the Adjusted R-Squared value.\n",
    "    \n",
    "    Adjusted R-squared provides an adjustment to the R-squared statistic such that an independent variable that has a correlation to Y increases adjusted R-squared and any variable without a strong correlation will make adjusted  R-squared decrease.\n",
    "\n",
    "\n",
    "\n",
    "#### Comparing the Observed Values :\n",
    "    Engagement and Phone Non Use gives the best Adjusted R-Squared values implying that these features best model the data.\n",
    "    Physical Activity, as the mode of the data was always stationary, it does not act as a good feature to fit the dataset. Hence, it has lower Adjusted R Squared Value\n",
    "    \n",
    "#### Survey Results\n",
    "    Using the aggregate features of all surveys gives an overfitted model. This is because the input rank is higher than the number of observations,i.e the number of features (65) is greater than the number of observations (27).\n",
    "    As a result of this, the Adjusted R Squared value is nan (Not a Number)\n",
    "    \n",
    "#### Aditional tuning of  survey features:\n",
    "\n",
    "    We experiment with 2 modifications of the survey features:\n",
    "    \n",
    "        1. Using PCA (Principal Component Analysis) \n",
    "           By reducing the number of dimensions to a value less than the number of observations, we do not observe an increased Adjusted R Squared value.\n",
    "        \n",
    "        2. Using fewer survey questionnaire types:\n",
    "           With increasing number of features < no of observations we observe increasing Adjusted R Squared values\n",
    "        \n",
    "       \n",
    "\n",
    "### 2) Discuss your rationale behind why one model (one feature category type) performs better than the other?\n",
    "\n",
    "    The Wifi Location, Physical Activity , Engagement, Phone Non Use and Survey are the 5 features.\n",
    "    \n",
    "    * Physical Activity of an individual, in this dataset, cannot predict if a user would perform better in an exam or not. The expectation is that increased physical activity implies a healthier lifestyle and this can lead to better performance in exams. However, most of the users in the dataset show similiar physical activity. Hence, this feature cannot be a good predictor of the grades. \n",
    "    \n",
    "    * WiFi Location is based on location of a user on the campus, and hence it may have a direct impact on academic performance. \n",
    "    \n",
    "    * Engagement and Phone Use usually are extremely good predictors of grades. As shown in studies, cell phone is significantly and negatively related to grades. As cell phone use goes up, test scores go down. Just as higher cell phone use predicts lower test scores, lower cell phone use also predicts higher scores.\n",
    "    \n",
    "__Source : http://www.longwood.edu/2015releases_62426.htm__\n",
    "\n",
    "    * Survey results show pre and post values of stress level questions, and hence it does not give a proper representation of the variation of stress values during the entire period of the study.Also, there were many missing post study completion result which makes it harder to acquire accurate results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
